{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microtuble Analysis Project - Track Feature Extraction\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Used for storing tiff movie series and images\n",
    "import pims\n",
    "\n",
    "#Used to analyze and extract features from images\n",
    "from skimage import io\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# Used to analyze trajectories\n",
    "import trackpy as tp\n",
    "import math\n",
    "\n",
    "# Used for motion analysis\n",
    "from scipy.spatial import distance\n",
    "from numpy import linalg as LA\n",
    "\n",
    "#Used for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identifyCellFromTrack(track_df, lab):\n",
    "    cell_ids = np.unique(lab)\n",
    "    cell_tracks = {}\n",
    "    for i in range(len(cell_ids)):\n",
    "        cell_tracks[i] = []\n",
    "    \n",
    "    for index,rows in track_df.iterrows():\n",
    "        try:\n",
    "            particle_coord_x = int(np.floor(rows['x']))\n",
    "            particle_coord_y = int(np.floor(rows['y']))\n",
    "            label_id = lab[particle_coord_y,particle_coord_x]\n",
    "            cell_tracks[label_id].append(rows['particle'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    all_tracks = []    \n",
    "    tracks_to_remove = []\n",
    "    \n",
    "    for i in range(len(cell_ids)):\n",
    "        cell_tracks[i] = list(set(cell_tracks[i]))\n",
    "        all_tracks.extend(cell_tracks[i])\n",
    "        if i==0:\n",
    "            tracks_to_remove.extend(cell_tracks[i])\n",
    "    \n",
    "    dupes = set([x for x in all_tracks if all_tracks.count(x) > 1])\n",
    "    tracks_to_remove.extend(dupes)\n",
    "    tracks_to_remove.extend(np.setdiff1d(list(track_df.particle), all_tracks))\n",
    "    tracks_to_remove = set(tracks_to_remove)\n",
    "    new_df = track_df[~track_df['particle'].isin(tracks_to_remove)]\n",
    "    new_df.index = range(len(new_df))\n",
    "    new_df['cell_id'] = np.nan\n",
    "    \n",
    "    reverse_dict = {}\n",
    "    for key,val in cell_tracks.items():\n",
    "        for item in val:\n",
    "            reverse_dict[item] = key\n",
    "\n",
    "    for index,rows in new_df.iterrows():\n",
    "        new_df.loc[index, 'cell_id'] = reverse_dict[rows['particle']]\n",
    "        reverse_dict[rows['particle']]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method: Generate feature dataframe that is empty\n",
    "def generate_features_df(track_df):\n",
    "    unique_tracks = np.array(track_df['particle'].unique())\n",
    "    new_df = pd.DataFrame(columns=['ID'])\n",
    "    for row_num, track_id in enumerate(unique_tracks):\n",
    "        new_df.loc[row_num] = [track_id]\n",
    "    new_df = new_df.set_index('ID')\n",
    "    return new_df\n",
    "\n",
    "# Add motions features to the features dataframe generated from method above\n",
    "def calc_motion_features(track_df, traj_feature_df):\n",
    "    new_df = pd.concat([traj_feature_df, \n",
    "                        pd.DataFrame(columns=['speed', 'speed_stdev', 'new_speed', 'new_speed_stdev', 'new_speed_range',\n",
    "                                              'new_speed_min', 'new_speed_max','curvature', 'curv_stdev', \n",
    "                                              'net_displacement', 'new_displacement', 'path_length', 'persistance', \n",
    "                                              'new_pathlength', 'new_persistance'])])\n",
    "    unique_tracks = np.array(track_df['particle'].unique())\n",
    "    \n",
    "    for row_num, track_id in enumerate(unique_tracks):\n",
    "        temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "\n",
    "        # Compute Vel X, Vel Y, and Speed\n",
    "        vel_x = np.gradient(temp_df['x'])\n",
    "        vel_y = np.gradient(temp_df['y'])\n",
    "        velocity = np.transpose(np.array([vel_x, vel_y]))\n",
    "        speed = [np.sqrt(dx**2 + vel_y[i]**2) for i,dx in enumerate(vel_x)]\n",
    "        \n",
    "        # Compute Normalized Speed\n",
    "        new_vel_x = np.gradient(temp_df['new_x'])\n",
    "        new_vel_y = np.gradient(temp_df['new_y'])\n",
    "        framerate = np.array(temp_df['delta_t'])\n",
    "        new_velocity = np.transpose(np.array([new_vel_x, new_vel_y]))\n",
    "        new_speed = [np.sqrt(dx**2 + new_vel_y[i]**2) for i,dx in enumerate(new_vel_x)]\n",
    "        new_speed = np.divide(new_speed, framerate)\n",
    "                              \n",
    "        # Compute the Acceleration Vector\n",
    "        #acc_x = np.gradient(vel_x)\n",
    "        #acc_y = np.gradient(vel_y)\n",
    "        #acceleration = np.transpose(np.array([acc_x, acc_y]))\n",
    "\n",
    "        # Compute Unit Norm and Unit Tangent Vectors \n",
    "        #unit_tangent = np.multiply(np.transpose(np.repeat([np.divide(1,speed)],2, axis=0)),velocity)\n",
    "\n",
    "        #norm = np.transpose(np.array([np.gradient(unit_tangent[:,0]), np.gradient(unit_tangent[:,1])]))\n",
    "        #norm_magnitude = [np.linalg.norm(vector) for vector in norm]\n",
    "        #unit_norm = np.multiply(np.transpose(np.repeat([np.divide(1,norm_magnitude)],2, axis=0)),norm)\n",
    "\n",
    "        # Compute Kurvature\n",
    "        #curv = np.divide(norm_magnitude, speed)\n",
    "        \n",
    "        #Compute Curvature again\n",
    "        ind_to_remove = np.logical_and(np.isnan(temp_df['x']), np.isnan(temp_df['y']))\n",
    "        temp_x = temp_df['x'][~ind_to_remove]\n",
    "        temp_y = temp_df['y'][~ind_to_remove]\n",
    "        t = np.arange(0,len(temp_x))\n",
    "    \n",
    "        z_x = np.polyfit(t, temp_x, 3)\n",
    "        z_y = np.polyfit(t, temp_y, 3)\n",
    "        z_dx = np.polyder(z_x)\n",
    "        z_ddx = np.polyder(z_dx)\n",
    "        z_dy = np.polyder(z_y)\n",
    "        z_ddy = np.polyder(z_dy)\n",
    "        \n",
    "        f_dx = np.poly1d(z_dx)\n",
    "        f_ddx = np.poly1d(z_ddx)\n",
    "        f_dy = np.poly1d(z_dy)\n",
    "        f_ddy = np.poly1d(z_ddy)\n",
    "        \n",
    "        t2 = np.linspace(1, len(temp_x)-2, 10)\n",
    "\n",
    "        dx = f_dx(t2)\n",
    "        ddx = f_ddx(t2)\n",
    "        dy = f_dy(t2)\n",
    "        ddy = f_ddy(t2)\n",
    "        \n",
    "        curv = np.divide(np.sqrt((ddy*dx - ddx*dy)**2),np.power((dx**2 + dy**2),1.5))\n",
    "        #curv = curv[1:-1]\n",
    "\n",
    "        #dx = np.gradient(temp_df['x'])\n",
    "        #dy = np.gradient(temp_df['y'])\n",
    "        #ddx = np.gradient(dx)\n",
    "        #ddy = np.gradient(dy)\n",
    "        \n",
    "        #curv = np.sqrt(np.divide(((ddy*dx - ddx*dy)**2),np.power((dx**2 + dy**2),1.5)))\n",
    "        \n",
    "        # Compute Net Displacement\n",
    "        first_val = temp_df.iloc[0]\n",
    "        last_val = temp_df.iloc[len(temp_df)-1]\n",
    "        net_displacement = np.sqrt(np.power(last_val['x'] - first_val['x'],2) + np.power(last_val['y'] - first_val['y'],2))\n",
    "        new_displacement = np.sqrt(np.power(last_val['new_x'] - first_val['new_x'],2) + np.power(last_val['new_y'] - first_val['new_y'],2))\n",
    "        \n",
    "        # Compute Total Path length\n",
    "        dx_squared = np.power(np.diff(temp_df['x']),2)\n",
    "        dy_squared = np.power(np.diff(temp_df['y']),2)\n",
    "        path_length = sum(np.sqrt(dx_squared+dy_squared))\n",
    "\n",
    "        #Compute New Path Length\n",
    "        new_dx_squared = np.power(np.diff(temp_df['new_x']),2)\n",
    "        new_dy_squared = np.power(np.diff(temp_df['new_y']),2)\n",
    "        new_pathlength = sum(np.sqrt(new_dx_squared+new_dy_squared))\n",
    "\n",
    "        # Compute Persistence\n",
    "        persistance = np.divide(net_displacement, path_length)    \n",
    "        new_persistance = np.divide(new_displacement, new_pathlength)\n",
    "        # Filter outliers of Acceleration/Speed\n",
    "        #speed_sorted = sorted(speed)\n",
    "        #speed_sorted_filtered = speed_sorted[math.floor(len(speed)/5):len(speed)-math.floor(len(speed)/5)]\n",
    "\n",
    "        #curv_sorted = sorted(curv)\n",
    "        #curv_sorted_filtered = curv_sorted[math.floor(len(curv)/5):len(curv)-math.floor(len(curv)/5)]\n",
    "\n",
    "        # Compute Average Acceleration/Speed\n",
    "        avg_speed = np.average(speed)\n",
    "        stdev_speed = np.std(speed)\n",
    "        avg_curv = np.max(curv)\n",
    "        stdev_curv = np.std(curv)\n",
    "        new_avg_speed = np.average(new_speed)\n",
    "        new_stdev_speed = np.std(new_speed)\n",
    "                              \n",
    "        # Add values to dataframe\n",
    "        new_df.loc[track_id]['speed'] = avg_speed\n",
    "        new_df.loc[track_id]['speed_stdev'] = stdev_speed\n",
    "        new_df.loc[track_id]['new_speed'] = new_avg_speed\n",
    "        new_df.loc[track_id]['new_speed_stdev'] = new_stdev_speed\n",
    "        new_df.loc[track_id]['curvature'] = avg_curv\n",
    "        new_df.loc[track_id]['curv_stdev'] = stdev_curv\n",
    "        new_df.loc[track_id]['net_displacement'] = net_displacement\n",
    "        new_df.loc[track_id]['path_length'] = path_length\n",
    "        new_df.loc[track_id]['persistance'] = persistance\n",
    "        new_df.loc[track_id]['new_displacement'] = new_displacement\n",
    "        new_df.loc[track_id]['new_pathlength'] = new_pathlength\n",
    "        new_df.loc[track_id]['new_persistance'] = new_persistance\n",
    "        new_df.loc[track_id]['new_speed_range'] = np.max(new_speed) - np.min(new_speed)\n",
    "        new_df.loc[track_id]['new_speed_min'] = np.min(new_speed)\n",
    "        new_df.loc[track_id]['new_speed_max'] = np.max(new_speed)\n",
    "    return new_df\n",
    "\n",
    "# Add the particle mass feature to the features dataframe \n",
    "def calc_mass_features(track_df, traj_feature_df):\n",
    "# Parse CSV file to find amplitude average + stdev\n",
    "    unique_tracks = np.array(track_df['particle'].unique())\n",
    "    new_df = pd.concat([traj_feature_df, \n",
    "                        pd.DataFrame(columns=['avg_mass', 'stdev_mass'])])\n",
    "\n",
    "    for i, track_id in enumerate(unique_tracks):\n",
    "        temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "\n",
    "        mass = np.array(temp_df['mass'])\n",
    "        mass_sorted = sorted(mass)\n",
    "        mass_sorted_filtered = mass_sorted[math.floor(len(mass)/5):len(mass)-math.floor(len(mass)/5)]\n",
    "\n",
    "        mass_avg = np.average(mass_sorted_filtered)\n",
    "        mass_stdev = np.std(mass_sorted_filtered)\n",
    "\n",
    "        new_df.loc[track_id]['avg_mass'] = mass_avg\n",
    "        new_df.loc[track_id]['stdev_mass'] = mass_stdev\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function Filters the track matrix (ie. t1,t2) using features\n",
    "def filter_track_from_feat(track_df, feature_df, feature_name, min_val=-np.inf, max_val=np.inf):\n",
    "    tracks_to_remove = []\n",
    "    for index,vals in feature_df.iterrows():\n",
    "        if (vals[feature_name] < min_val or vals[feature_name]>max_val):\n",
    "            tracks_to_remove.append(index)\n",
    "    #print (\"deleting following tracks:\" , tracks_to_remove)\n",
    "    new_df = track_df[~track_df['particle'].isin(tracks_to_remove)]\n",
    "    return new_df    \n",
    "    \n",
    "#Function filteres the trajectory feature matrix using features\n",
    "def filter_traj_from_feat(feature_df, feature_name, min_val=-np.inf, max_val=np.inf):\n",
    "    tracks_to_remove = []\n",
    "    for index,vals in feature_df.iterrows():\n",
    "        if (vals[feature_name] < min_val or vals[feature_name]>max_val):\n",
    "            tracks_to_remove.append(index)\n",
    "    #print (\"deleting following tracks:\" , tracks_to_remove)\n",
    "    new_df = feature_df[~feature_df.index.isin(tracks_to_remove)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(x1,y1, x2,y2, x3,y3): # x3,y3 is the point\n",
    "    px = x2-x1\n",
    "    py = y2-y1\n",
    "    something = px*px + py*py\n",
    "    u =  ((x3 - x1) * px + (y3 - y1) * py) / float(something)\n",
    "    if u > 1:\n",
    "        u = 1\n",
    "    elif u < 0:\n",
    "        u = 0\n",
    "    x = x1 + u * px\n",
    "    y = y1 + u * py\n",
    "    dx = x - x3\n",
    "    dy = y - y3\n",
    "    dist = math.sqrt(dx*dx + dy*dy)\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distance Functions\n",
    "# Gets the distance between the axis and a specific track\n",
    "def getDistanceFromAxis(track_df, track_id,  cell_ID=1, axis='major', label_img=None,):\n",
    "    props = regionprops(label_img)[cell_ID-1]\n",
    "    y1, x1 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x2 = np.nan\n",
    "    y2 = np.nan\n",
    "    if axis=='major':\n",
    "        x2 = x1 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "        y2 = y1 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "    else:\n",
    "        x2 = x1 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "        y2 = y1 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "        \n",
    "    temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "    distances = []\n",
    "    for index,rows in temp_df.iterrows():\n",
    "        x0 = rows['x']\n",
    "        y0 = rows['y']\n",
    "\n",
    "        num = np.absolute(np.multiply(y2-y1, x0) - np.multiply(x2-x1, y0)+ np.multiply(x2,y1) - np.multiply(y2,x1))\n",
    "        den = np.sqrt(np.power(y2-y1,2) + np.power(x2-x1,2))\n",
    "        distance = np.divide(num,den)\n",
    "        distances.append(distance)\n",
    "        \n",
    "    return min(distances)\n",
    "\n",
    "\n",
    "#Gets the signed distance between an axis and a specific cell (MIGHT NOT NEED THIS)\n",
    "def getsignedDistanceFromAxis(track_df, track_id, label_img=None, cell_ID=1, axis='major'):\n",
    "    props = regionprops(label_img)[cell_ID-1]\n",
    "    y1, x1 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x2 = np.nan\n",
    "    y2 = np.nan\n",
    "    if axis=='major':\n",
    "        x2 = x1 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "        y2 = y1 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "    else:\n",
    "        x2 = x1 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "        y2 = y1 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "        \n",
    "    temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "    distances = []\n",
    "    for index,rows in temp_df.iterrows():\n",
    "        x0 = rows['x']\n",
    "        y0 = rows['y']\n",
    "        #print(index,x0,y0)\n",
    "\n",
    "        num = np.multiply(y2-y1, x0) - np.multiply(x2-x1, y0)+ np.multiply(x2,y1) - np.multiply(y2,x1)\n",
    "        den = np.sqrt(np.power(y2-y1,2) + np.power(x2-x1,2))\n",
    "        distance = np.divide(num,den)\n",
    "        distances.append(distance)\n",
    "        \n",
    "    return min(distances)\n",
    "\n",
    "# Compute Distance of a track from the center of the cell\n",
    "def getDistanceFromCenter(track_df, track_id, cell_ID=1, label_img=None):\n",
    "    distances = [] \n",
    "    y0, x0 = regionprops(img_label)[cell_ID-1].centroid\n",
    "    centroid = np.array([x0,y0])\n",
    "    temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "    for index,rows in temp_df.iterrows():\n",
    "        object_coord = np.array([rows['x'],rows['y']])\n",
    "        try:\n",
    "            dst = distance.euclidean(object_coord, centroid)\n",
    "        except:\n",
    "            dst = np.inf\n",
    "        distances.append(dst)\n",
    "    return min(distances)\n",
    "\n",
    "# Function Identifies nearby tracks given the track dataframe, a track of interest, and a search radius\n",
    "def getDistanceFromTrack(track_df, track_id, poi_id):\n",
    "    poi_df = track_df.loc[track_df['particle'] == poi_id]\n",
    "    temp_df = track_df.loc[track_df['particle'] == track_id]\n",
    "    distances = []\n",
    "    for index,rows in temp_df.iterrows():\n",
    "        object_coord = np.array([rows['new_x'],rows['new_y']])\n",
    "        for index_p, rows_p in poi_df.iterrows():\n",
    "            poi_coord = np.array([rows_p['new_x'], rows_p['new_y']])\n",
    "            try:\n",
    "                dst = distance.euclidean(object_coord, poi_coord)\n",
    "\n",
    "            except:\n",
    "                dst = np.inf\n",
    "            \n",
    "            distances.append(dst)            \n",
    "    return min(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNearbyTrackstoObject(track_df, func, cell_ID=1, min_val = 0, max_val=30, **kwargs):\n",
    "    nearby_tracks = []\n",
    "    unique_tracks = np.array(track_df['particle'].unique())\n",
    "    val = np.infty\n",
    "    remove=False\n",
    "    for track_id in unique_tracks:\n",
    "        if 'axis' in kwargs:\n",
    "            val = func(track_df, track_id, cell_ID, axis=kwargs['axis'], label_img=kwargs['label'])\n",
    "        elif 'poi_id' in kwargs:\n",
    "            val = func(track_df, track_id, poi_id=kwargs['poi_id'])\n",
    "            remove=True\n",
    "        else:\n",
    "            val = func(track_df, track_id, cell_ID, label_img=kwargs['label'])\n",
    "        if val <= max_val and val >= min_val:\n",
    "            nearby_tracks.append(track_id)\n",
    "    if remove == True:\n",
    "        nearby_tracks.remove(kwargs['poi_id'])\n",
    "    return nearby_tracks\n",
    "\n",
    "def createTrackDict(track_df, min_search=10, max_search=200, step_size=30, *track_dict, **kwargs):\n",
    "    return_dict = {}\n",
    "    if track_dict:\n",
    "        return_dict = track_dict\n",
    "    \n",
    "    if 'label' in kwargs:\n",
    "        for i in range(min_search,max_search,step_size):\n",
    "            print(i, end=\" \")\n",
    "            temp_tracks = getNearbyTrackstoObject(t3, max_val=i, **kwargs)\n",
    "            return_dict[i] = temp_tracks\n",
    "    else:    \n",
    "        unique_tracks = np.array(track_df['particle'].unique())   \n",
    "        for track_id in unique_tracks:\n",
    "            return_dict[track_id] = {}\n",
    "            print(track_id, end=\" \")\n",
    "            for i in range(min_search,max_search,step_size):\n",
    "                temp_tracks = getNearbyTrackstoObject(t3, poi_id= track_id, max_val=i, **kwargs)\n",
    "                return_dict[track_id][i] = temp_tracks\n",
    "    return return_dict\n",
    "\n",
    "def create_distance_matrix(track_df, unique_tracks):\n",
    "    return_mat = np.zeros([len(unique_tracks), len(unique_tracks)])\n",
    "    for i, track_id in enumerate(unique_tracks):\n",
    "        for j, track_id_2 in enumerate(unique_tracks):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if return_mat[i,j] != 0:\n",
    "                continue\n",
    "            else:\n",
    "                dst = getDistanceFromTrack(track_df, track_id_2, track_id)\n",
    "                return_mat[i, j] = dst\n",
    "                return_mat[j, i] = dst\n",
    "    return return_mat\n",
    "\n",
    "#dist_mat = create_distance_matrix(temp_cell_df, np.array(temp_cell_df['particle'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function computes the cosine distance between the track of interest and other tracks specified in an array\n",
    "def compute_cosine_distance(track_df, poi_id, nearby_tracks):\n",
    "    # Compute the Cosine Distance to find differences in orientation between all these tracks\n",
    "    cosine_distances = []\n",
    "    displacment_vector = []\n",
    "    \n",
    "    if isinstance(poi_id, (np.ndarray)):\n",
    "        displacment_vector = poi_id\n",
    "    else:\n",
    "        poi_df = track_df.loc[track_df['particle'] == poi_id]\n",
    "        #Compute Displacement vector of the particle of interest\n",
    "        min_particle = poi_df.loc[poi_df['frame'].argmin()]\n",
    "        max_particle = poi_df.loc[poi_df['frame'].argmax()]\n",
    "        min_coord = np.array([min_particle['x'], min_particle['y']])\n",
    "        max_coord = np.array([max_particle['x'], max_particle['y']])\n",
    "        displacment_vector = np.subtract(max_coord, min_coord)        \n",
    "    \n",
    "    for particle_id in nearby_tracks:\n",
    "        temp_df = track_df.loc[track_df['particle'] == particle_id]\n",
    "        min_particle = temp_df.loc[temp_df['frame'].argmin()]\n",
    "        max_particle = temp_df.loc[temp_df['frame'].argmax()]\n",
    "        min_coord = np.array([min_particle['x'], min_particle['y']])\n",
    "        max_coord = np.array([max_particle['x'], max_particle['y']])\n",
    "        disp_vector_2 = np.subtract(max_coord, min_coord)\n",
    "\n",
    "        cos_dist = np.divide(np.dot(displacment_vector, disp_vector_2), LA.norm(displacment_vector) * LA.norm(disp_vector_2))\n",
    "        cosine_distances.append([cos_dist, particle_id])\n",
    "    return cosine_distances\n",
    "\n",
    "def getOrientationFromAxis(track_df, track_id, cell_ID=1, label_img=None, axis='major'):\n",
    "    props = regionprops(label_img)[cell_ID-1]\n",
    "    y0, x0 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    \n",
    "    axis_vec = np.nan\n",
    "    if axis=='major':\n",
    "        x1 = x0 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "        y1 = y0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "        axis_vec = np.array([x1-x0, y1-y0])\n",
    "    else:\n",
    "        x2 = x0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "        y2 = y0 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "        axis_vec = np.array([x2-x0, y2-y0])\n",
    "\n",
    "    cosine_distance = compute_cosine_distance(nearby_tracks=[track_id], track_df=track_df, poi_id=axis_vec)\n",
    "    return cosine_distance[0][0]\n",
    "\n",
    "def getOrientationTracksAxis(track_df, min_val=0, max_val=1, cell_ID=1, label_img=None, axis='major'):\n",
    "    nearby_tracks = []\n",
    "    unique_tracks = np.array(track_df['particle'].unique())\n",
    "    for track_id in unique_tracks:\n",
    "        val = getOrientationFromAxis(track_df, track_id, cell_ID, label_img, axis='major')\n",
    "        if val <= max_val and val >= min_val:\n",
    "            nearby_tracks.append(track_id)\n",
    "    return nearby_tracks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = ['3_23_17_invitro', '11_10_16_invitro', '0836_M17_EB3-appple_4_3_2017_left_ear', \n",
    "           '0836_M19_EB3-apple_4_12_2017_no_hole', '0836_M20_EB3-apple_tubulin_MertK-GFP_4_12_2017_right_ear',\n",
    "          'B8P1', 'C5P1', 'E7P2_1', 'E7P2_2_', 'F8P1_1', 'F8P1_2', 'F10P2', 'invivo_1']\n",
    "\n",
    "folders = ['collagen_eb3_plate1', 'collagen_eb3_plate2']\n",
    "folders = ['invivo_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: invivo_2\n",
      "\tt1_tracks: removing tracks less than a certain length\n",
      "\tt2_tracks: identify the cell for each track\n",
      "\tt2_features: Computing Motion and mass features from each track\n",
      "\tt3_tracks and t3_features: filtering tracks based on features values\n"
     ]
    }
   ],
   "source": [
    "#PHASE 1\n",
    "for folder in folders:\n",
    "    ##################################################\n",
    "    # Load all track data\n",
    "    root_dir = \"../../filtered_data_2/\" + folder + \"/\"\n",
    "    track_dir = root_dir + \"track_matrices\"\n",
    "    movie_dir = root_dir + \"movie_data\"\n",
    "    mask_dir = root_dir + \"masks\"\n",
    "    pickle_dir = root_dir + \"pickle_objs/\"\n",
    "\n",
    "    file_names = os.listdir(track_dir)\n",
    "    \n",
    "    movies = []\n",
    "    img_sequences = []\n",
    "    label_imgs = []\n",
    "    mask_imgs = []\n",
    "    movie_names = []\n",
    "    t0_tracks = []\n",
    "    \n",
    "    print(\"Loading data: \" + folder)\n",
    "    img_id = 0\n",
    "    for name in file_names:\n",
    "        new_name = name.split(\".csv\")[0]\n",
    "        try:\n",
    "            #Loading Movie data\n",
    "            img_sequence = io.imread(movie_dir+\"/\"+new_name+\".tif\")\n",
    "            movies.append(pims.Frame(img_sequence))\n",
    "            img_sequences.append(img_sequence)\n",
    "            movie_names.append(name)\n",
    "            #Loading Mask Data\n",
    "            img_mask = io.imread(mask_dir+\"/\"+new_name+\".tif\")\n",
    "            mask_imgs.append(img_mask)\n",
    "            label_imgs.append(label(img_mask))\n",
    "            #Loading Track Data as dataframes\n",
    "            t = pd.read_csv(track_dir+\"/\"+new_name+\".csv\", header=None)\n",
    "            t.columns = [\"x\", \"y\", \"mass\", \"frame\", \"particle\"]\n",
    "            t['img_id'] = img_id\n",
    "            t0_tracks.append(t)\n",
    "            img_id+=1\n",
    "        except:\n",
    "            print(new_name + \" was not found\")\n",
    "            \n",
    "    pickle.dump(file_names, open(pickle_dir + \"file_names.p\", \"wb\"))\n",
    "    \n",
    "    ##################################################\n",
    "    print(\"\\tt1_tracks: removing tracks less than a certain length\")\n",
    "    t1_tracks = []\n",
    "    for i,temp_df in enumerate(t0_tracks):\n",
    "        num_frames = img_sequences[i].shape[0]\n",
    "        run_length = 3\n",
    "        if num_frames >40:\n",
    "            run_length = 5\n",
    "        t1 = tp.filter_stubs(temp_df, run_length)\n",
    "        t1_tracks.append(t1) \n",
    "    #print('Before:', temp_df['particle'].nunique(), 'After:',t1['particle'].nunique())\n",
    "\n",
    "    ##################################################\n",
    "    print(\"\\tt2_tracks: identify the cell for each track\")\n",
    "    t2_tracks = []\n",
    "    for i,temp_df in enumerate(t1_tracks):\n",
    "        #print(\"Identifying Cells for Track\" , i)\n",
    "        t2 = identifyCellFromTrack(t1_tracks[i], label_imgs[i])\n",
    "        t2_tracks.append(t2) \n",
    "\n",
    "    res_framerate_file = root_dir + \"external_data/image_resolution_framerate.csv\"\n",
    "    res_framerate = pd.read_csv(res_framerate_file, header=None)\n",
    "\n",
    "    t2_2_tracks = []\n",
    "    for i,temp_df in enumerate(t2_tracks):\n",
    "        row_val = res_framerate.loc[res_framerate[0] == file_names[i]]\n",
    "        new_df = pd.concat([temp_df, \n",
    "                        pd.DataFrame(columns=['new_x', 'new_y','new_frame', 'delta_t', 'resolution'])])\n",
    "        res = list(row_val[1])[0]\n",
    "        framerate = list(row_val[2])[0]\n",
    "        new_x = np.divide(temp_df['x'], res)\n",
    "        new_y = np.divide(temp_df['y'], res)\n",
    "        new_frame = np.multiply(temp_df['frame'], framerate)\n",
    "        new_df['new_x'] = new_x\n",
    "        new_df['new_y'] = new_y\n",
    "        new_df['new_frame'] = new_frame\n",
    "        new_df['delta_t'] = framerate\n",
    "        new_df['resolution'] = res\n",
    "        t2_2_tracks.append(new_df)\n",
    "\n",
    "    t2_tracks = t2_2_tracks\n",
    "    pickle.dump(t2_tracks, open(pickle_dir + \"t2_tracks.p\", \"wb\"))\n",
    "    \n",
    "    ##################################################\n",
    "    print(\"\\tt2_features: Computing Motion and mass features from each track\")   \n",
    "    t2_features = []\n",
    "    for i,temp_df in enumerate(t2_tracks):\n",
    "        #print(\"Calculating Basic Features for Track\", i)\n",
    "        t2_feat = generate_features_df(temp_df)\n",
    "        t2_feat = calc_mass_features(temp_df, t2_feat)\n",
    "        t2_feat = calc_motion_features(temp_df, t2_feat)\n",
    "        t2_features.append(t2_feat)\n",
    "        \n",
    "    pickle.dump(t2_features, open(pickle_dir + \"t2_features.p\", \"wb\"))\n",
    "    \n",
    "    #t2_tracks = pickle.load(open(pickle_dir + \"t2_tracks.p\", \"rb\"))\n",
    "    #t2_features = pickle.load(open(pickle_dir + \"t2_features.p\", \"rb\"))\n",
    "    \n",
    "    ##################################################\n",
    "    print(\"\\tt3_tracks and t3_features: filtering tracks based on features values\")\n",
    "    t3_tracks = []\n",
    "    t3_features = []\n",
    "    for i,temp_df in enumerate(t2_tracks):\n",
    "        #print(\"Filtering based on Features for Track\", i)\n",
    "        t3 = temp_df\n",
    "        t3_feat = t2_features[i]\n",
    "        t3 = filter_track_from_feat(t3, t3_feat, 'new_displacement', min_val=1)\n",
    "        t3_feat = filter_traj_from_feat(t3_feat, 'new_displacement', min_val=1)\n",
    "\n",
    "        t3 = filter_track_from_feat(t3, t3_feat, 'new_persistance', min_val=0.60)\n",
    "        t3_feat = filter_traj_from_feat(t3_feat, 'new_persistance', min_val=0.60)\n",
    "        #t3 = filter_track_from_feat(t3, t3_feat, 'curvature', max_val = 5)\n",
    "        #t3_feat = filter_traj_from_feat(t3_feat, 'curvature', max_val = 5)\n",
    "\n",
    "        t3_tracks.append(t3)\n",
    "        t3_features.append(t3_feat)\n",
    "\n",
    "    pickle.dump(t3_tracks, open(pickle_dir + \"t3_tracks.p\", \"wb\"))\n",
    "    pickle.dump(t3_features, open(pickle_dir + \"t3_features.p\", \"wb\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: invivo_2\n",
      "\tt3_tracks_updated: Calculating Distance Feature\n",
      "\tcreating track dictionaries\n",
      "\tt4_features - Get similarity features\n",
      "\tt5_features - Get Orientation Features\n",
      "\tt6_features - Creating Cell IDs\n"
     ]
    }
   ],
   "source": [
    "#PHASE 2\n",
    "for folder in folders:\n",
    "    ##################################################\n",
    "    # Load all track data\n",
    "    root_dir = \"../../filtered_data_2/\" + folder + \"/\"\n",
    "    track_dir = root_dir + \"track_matrices\"\n",
    "    movie_dir = root_dir + \"movie_data\"\n",
    "    mask_dir = root_dir + \"masks\"\n",
    "    pickle_dir = root_dir + \"pickle_objs/\"\n",
    "    file_names = pickle.load(open(pickle_dir + \"file_names.p\", \"rb\"))\n",
    "    \n",
    "    movies = []\n",
    "    img_sequences = []\n",
    "    label_imgs = []\n",
    "    mask_imgs = []\n",
    "    movie_names = []\n",
    "    t0_tracks = []\n",
    "    \n",
    "    print(\"Loading data: \" + folder)\n",
    "    img_id = 0\n",
    "    for name in file_names:\n",
    "        new_name = name.split(\".csv\")[0]\n",
    "        try:\n",
    "            #Loading Movie data\n",
    "            img_sequence = io.imread(movie_dir+\"/\"+new_name+\".tif\")\n",
    "            movies.append(pims.Frame(img_sequence))\n",
    "            img_sequences.append(img_sequence)\n",
    "            movie_names.append(name)\n",
    "            #Loading Mask Data\n",
    "            img_mask = io.imread(mask_dir+\"/\"+new_name+\".tif\")\n",
    "            mask_imgs.append(img_mask)\n",
    "            label_imgs.append(label(img_mask))\n",
    "            #Loading Track Data as dataframes\n",
    "            t = pd.read_csv(track_dir+\"/\"+new_name+\".csv\", header=None)\n",
    "            t.columns = [\"x\", \"y\", \"mass\", \"frame\", \"particle\"]\n",
    "            t['img_id'] = img_id\n",
    "            t0_tracks.append(t)\n",
    "            img_id+=1\n",
    "        except:\n",
    "            print(new_name + \" was not found\")\n",
    "            \n",
    "    ##################################################\n",
    "    print(\"\\tt3_tracks_updated: Calculating Distance Feature\")\n",
    "    t3_tracks = pickle.load(open(pickle_dir + \"t3_tracks.p\", \"rb\"))\n",
    "    t3_features = pickle.load(open(pickle_dir + \"t3_features.p\", \"rb\"))\n",
    "    \n",
    "    #Computing edge distance\n",
    "    distance_transforms = []\n",
    "    for i,temp in enumerate(t3_tracks):\n",
    "        num_cells = len(np.unique(label_imgs[i])) - 1\n",
    "        new_dist_transform = []\n",
    "        for j in range(num_cells):\n",
    "            cell_id = j+1\n",
    "            temp_mask = np.multiply(mask_imgs[i],label_imgs[i] == cell_id)\n",
    "            dist_transf= distance_transform_edt(temp_mask)\n",
    "            distance_transf_sum = np.sum(dist_transf)\n",
    "            num_pixels = np.sum(temp_mask)\n",
    "            if cell_id == 1:\n",
    "                new_dist_transform = np.divide(dist_transf, (distance_transf_sum/num_pixels))\n",
    "            else:\n",
    "                new_dist_transform = np.add(new_dist_transform, np.divide(dist_transf, (distance_transf_sum/num_pixels)))\n",
    "        distance_transforms.append(new_dist_transform)\n",
    "    \n",
    "    t3_temp_features = []\n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        unique_tracks = np.array(temp_df['particle'].unique())\n",
    "        new_df = pd.concat([t3_features[i],pd.DataFrame(columns=['edge_distance'])])\n",
    "        for index, track_id in enumerate(unique_tracks):\n",
    "            track_particles = temp_df.loc[temp_df['particle'] == track_id]\n",
    "            #x and y coordinates are switched in distance trasnform for some reason\n",
    "            x_vals = np.mean(track_particles['x'])\n",
    "            y_vals = np.mean(track_particles['y'])\n",
    "            result = distance_transforms[i][int(y_vals),int(x_vals)]   \n",
    "            new_df.loc[track_id]['edge_distance'] = result\n",
    "        t3_temp_features.append(new_df)\n",
    "    t3_features = t3_temp_features\n",
    "    \n",
    "    #Min axis distance and Maj axis distance features\n",
    "    majaxis_transform = []\n",
    "    minaxis_transform = []\n",
    "    for i,temp in enumerate(t3_tracks):\n",
    "        num_cells = len(np.unique(label_imgs[i])) - 1\n",
    "        new_maj_transform = np.zeros((mask_imgs[i].shape[0],mask_imgs[i].shape[1]))\n",
    "        new_min_transform = np.zeros((mask_imgs[i].shape[0],mask_imgs[i].shape[1]))\n",
    "        for j in range(num_cells):\n",
    "            cell_id = j+1\n",
    "            temp_mask = np.multiply(mask_imgs[i],label_imgs[i] == cell_id)\n",
    "\n",
    "            props = regionprops(temp_mask)\n",
    "            props = props[0]\n",
    "            y0, x0 = props.centroid\n",
    "            orientation = props.orientation\n",
    "            x1_maj = x0 + math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "            y1_maj = y0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "            x2_maj = x0 - math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "            y2_maj = y0 + math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "\n",
    "            x1_min = x0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "            y1_min = y0 - math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "            x2_min = x0 + math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "            y2_min = y0 + math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "\n",
    "            new_mask_min = np.zeros((temp_mask.shape[0], temp_mask.shape[1]))\n",
    "            new_mask_maj = np.zeros((temp_mask.shape[0], temp_mask.shape[1]))\n",
    "\n",
    "            for x_pos in range(temp_mask.shape[0]):\n",
    "                for y_pos in range(temp_mask.shape[1]):\n",
    "                    new_mask_min[x_pos,y_pos] = dist(x1_min,y1_min,x2_min,y2_min,y_pos,x_pos)\n",
    "                    new_mask_maj[x_pos,y_pos] = dist(x1_maj,y1_maj,x2_maj,y2_maj,y_pos,x_pos)\n",
    "\n",
    "            new_mask_min = np.multiply(temp_mask,new_mask_min)\n",
    "            max_value = np.max(new_mask_min)\n",
    "            new_mask_min = np.divide(new_mask_min, max_value)\n",
    "\n",
    "            new_mask_maj = np.multiply(temp_mask,new_mask_maj)\n",
    "            max_value = np.max(new_mask_maj)\n",
    "            new_mask_maj = np.divide(new_mask_maj, max_value)\n",
    "\n",
    "            new_maj_transform = np.add(new_mask_maj, new_maj_transform)\n",
    "            new_min_transform = np.add(new_mask_min, new_min_transform)\n",
    "\n",
    "        majaxis_transform.append(new_maj_transform)\n",
    "        minaxis_transform.append(new_min_transform)\n",
    "        t3_temp_features = []\n",
    "        \n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        unique_tracks = np.array(temp_df['particle'].unique())\n",
    "        new_df = pd.concat([t3_features[i],pd.DataFrame(columns=['maj_axis_distance', 'min_axis_distance'])])\n",
    "        for index, track_id in enumerate(unique_tracks):\n",
    "            track_particles = temp_df.loc[temp_df['particle'] == track_id]\n",
    "            #x and y coordinates are switched in distance trasnform for some reason\n",
    "            x_vals = np.mean(track_particles['x'])\n",
    "            y_vals = np.mean(track_particles['y'])\n",
    "            result_major = majaxis_transform[i][int(y_vals),int(x_vals)]   \n",
    "            result_minor = minaxis_transform[i][int(y_vals),int(x_vals)]   \n",
    "            new_df.loc[track_id]['maj_axis_distance'] = result_major\n",
    "            new_df.loc[track_id]['min_axis_distance'] = result_minor\n",
    "        t3_temp_features.append(new_df)\n",
    "    t3_features = t3_temp_features\n",
    "    \n",
    "    pickle.dump(t3_tracks, open(pickle_dir + \"t3_tracks.p\", \"wb\"))\n",
    "    pickle.dump(t3_features, open(pickle_dir + \"t3_features.p\", \"wb\"))\n",
    "    \n",
    "    ##################################################\n",
    "    #t3_tracks = pickle.load(open(pickle_dir + \"t3_tracks.p\", \"rb\"))\n",
    "    #t3_features = pickle.load(open(pickle_dir + \"t3_features.p\", \"rb\"))\n",
    "    \n",
    "    print(\"\\tcreating track dictionaries\")\n",
    "    track_distance_dicts = {}\n",
    "\n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        track_distance_dicts[i] = {}\n",
    "        unique_cells =  np.array(temp_df['cell_id'].unique())\n",
    "\n",
    "        for cell in unique_cells:\n",
    "            #track_distance_dicts[i][cell] = {}\n",
    "            #print(\"Creating Dictionary for Cell\", cell, \"in Image\", i)\n",
    "            temp_cell_df = temp_df.loc[temp_df['cell_id'] == cell]\n",
    "            unique_tracks = np.array(temp_cell_df['particle'].unique())\n",
    "            dist_mat = create_distance_matrix(temp_cell_df, unique_tracks)\n",
    "\n",
    "            for index,track_id in enumerate(unique_tracks):\n",
    "                #track_distance_dicts[i][cell][track_id] = {}\n",
    "                track_distance_dicts[i][track_id] = {}\n",
    "\n",
    "                for srange in [5,10,20,100,200]:\n",
    "                    nearby_tracks = [unique_tracks[j] for j, val in enumerate(dist_mat[index,:]) \n",
    "                                     if (val<srange and j != index)]\n",
    "                    #track_distance_dicts[i][cell][track_id][srange] = nearby_tracks\n",
    "                    track_distance_dicts[i][track_id][srange] = nearby_tracks  \n",
    "                    \n",
    "    pickle.dump(track_distance_dicts, open(pickle_dir + \"track_distance_dicts.p\", \"wb\"))\n",
    "    #track_distance_dicts = pickle.load(open(pickle_dir + \"track_distance_dicts.p\", \"rb\"))\n",
    "    \n",
    "    ##################################################\n",
    "    # Get Features from Plots Above that Represent Orientation\n",
    "    print(\"\\tt4_features - Get similarity features\")\n",
    "\n",
    "    t4_features = []\n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        #print(\"Computing Similarity Features for Image\",  i)\n",
    "        unique_tracks = np.array(temp_df['particle'].unique())\n",
    "        new_df = pd.concat([t3_features[i],pd.DataFrame(columns=['similarity_5','similarity_10','similarity_20', 'similarity_100', 'similarity_200'])])\n",
    "        for index, track_id in enumerate(unique_tracks):\n",
    "            nearby_tracks = track_distance_dicts[i][track_id][5]\n",
    "            if nearby_tracks == []:\n",
    "                continue\n",
    "            else:\n",
    "                distances = compute_cosine_distance(temp_df, track_id, nearby_tracks)\n",
    "                average_distance = np.average(np.array(distances)[:,0])\n",
    "                new_df.loc[track_id]['similarity_5'] = average_distance\n",
    "\n",
    "            nearby_tracks = track_distance_dicts[i][track_id][10]\n",
    "            distances = compute_cosine_distance(temp_df, track_id, nearby_tracks)\n",
    "            average_distance = np.average(np.array(distances)[:,0])\n",
    "            new_df.loc[track_id]['similarity_10'] = average_distance\n",
    "\n",
    "            nearby_tracks = track_distance_dicts[i][track_id][20]\n",
    "            distances = compute_cosine_distance(temp_df, track_id, nearby_tracks)\n",
    "            average_distance = np.average(np.array(distances)[:,0])\n",
    "            new_df.loc[track_id]['similarity_20'] = average_distance\n",
    "\n",
    "            nearby_tracks = track_distance_dicts[i][track_id][100]\n",
    "            distances = compute_cosine_distance(temp_df, track_id, nearby_tracks)\n",
    "            average_distance = np.average(np.array(distances)[:,0])\n",
    "            new_df.loc[track_id]['similarity_100'] = average_distance\n",
    "\n",
    "            nearby_tracks = track_distance_dicts[i][track_id][200]\n",
    "            distances = compute_cosine_distance(temp_df, track_id, nearby_tracks)\n",
    "            average_distance = np.average(np.array(distances)[:,0])\n",
    "            new_df.loc[track_id]['similarity_200'] = average_distance\n",
    "        t4_features.append(new_df)\n",
    "        \n",
    "    pickle.dump(t4_features, open(pickle_dir + \"t4_features.p\", \"wb\"))\n",
    "    #t4_features = pickle.load(open(pickle_dir + \"t4_features.p\", \"rb\"))\n",
    "\n",
    "    ##################################################\n",
    "    print(\"\\tt5_features - Get Orientation Features\")\n",
    "    t5_features = []\n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        #print(\"Computing Major/Minor Axis Features for Image\",  i)\n",
    "        unique_tracks = np.array(temp_df['particle'].unique())\n",
    "        new_df = pd.concat([t4_features[i],pd.DataFrame(columns=['maj_orient', 'min_orient', 'maj_orient_mag', 'min_orient_mag'])])\n",
    "        for index, track_id in enumerate(unique_tracks):\n",
    "            cell_ID = int(temp_df.loc[temp_df['particle']==track_id]['cell_id'].unique()[0])\n",
    "            distance_maj = getOrientationFromAxis(temp_df, track_id, cell_ID=cell_ID, label_img=label_imgs[i], axis='major')\n",
    "            distance_min = getOrientationFromAxis(temp_df, track_id, cell_ID=cell_ID, label_img=label_imgs[i], axis='minor')\n",
    "            new_df.loc[track_id]['maj_orient'] = distance_maj\n",
    "            new_df.loc[track_id]['min_orient'] = distance_min\n",
    "            new_df.loc[track_id]['maj_orient_mag'] = np.absolute(distance_maj)\n",
    "            new_df.loc[track_id]['min_orient_mag'] = np.absolute(distance_min)\n",
    "\n",
    "        t5_features.append(new_df)\n",
    "        \n",
    "    t6_features = []\n",
    "    ##################################################\n",
    "    print(\"\\tt6_features - Creating Cell IDs\")\n",
    "    for i,temp_df in enumerate(t3_tracks):\n",
    "        #print(\"Creating Unique Cell ID for image\",  i)\n",
    "        unique_tracks = np.array(temp_df['particle'].unique())\n",
    "        new_df = pd.concat([t5_features[i],pd.DataFrame(columns=['unique_cell_id', 'unique_id', 'img_id'])])\n",
    "        for index, track_id in enumerate(unique_tracks):\n",
    "            cell_ID = int(temp_df.loc[temp_df['particle']==track_id]['cell_id'].unique()[0])\n",
    "            new_cell_id = float(str(i)+ '.'+str(cell_ID))\n",
    "            new_df.loc[track_id]['unique_cell_id'] = new_cell_id\n",
    "            new_part_id = float(str(i)+'.'+str(int(track_id)))\n",
    "            new_df.loc[track_id]['unique_id'] = new_part_id\n",
    "            new_df.loc[track_id]['img_id'] = i\n",
    "        t6_features.append(new_df.dropna())\n",
    "        \n",
    "    pickle.dump(t6_features, open(pickle_dir + \"t6_features.p\", \"wb\"))\n",
    "    #t6_features = pickle.load(open(pickle_dir + \"t6_features.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
